# Copyright AGNTCY Contributors (https://github.com/agntcy)
# SPDX-License-Identifier: Apache-2.0

import logging
import uuid
import os
from langchain_core.messages import AIMessage
from langgraph.graph.state import CompiledStateGraph
from langgraph.prebuilt import create_react_agent
from langgraph_supervisor import create_supervisor
from ioa_observe.sdk.decorators import agent, graph

from common.llm import get_llm
from graph.tools import FlavorProfileTool

from farm.card import AGENT_CARD as farm_agent_card

logger = logging.getLogger("corto.supervisor.graph")


@agent(name="exchange_agent")
class ExchangeGraph:
    def __init__(self):
        self.graph = self.build_graph()

    @graph(name="exchange_graph")
    def build_graph(self) -> CompiledStateGraph:
        """
        Constructs and compiles a LangGraph instance.

        This function initializes a `SupervisorAgent` to create the base graph structure
        and uses an `InMemorySaver` as the checkpointer for the compilation process.

        The resulting compiled graph can be used to execute Supervisor workflow in LangGraph Studio.

        Returns:
        CompiledGraph: A fully compiled LangGraph instance ready for execution.
        """
        model = get_llm()

        # initialize the flavor profile tool(used for coffee flavor, taste, or sensory profile estimation) with the farm agent card
        flavor_profile_tool = FlavorProfileTool(
            remote_agent_card=farm_agent_card,
        )
        
        #  worker agent- always responsible for flavor, taste, or sensory profile of coffee queries
        get_flavor_profile_a2a_agent = create_react_agent(
            model=model,
            tools=[flavor_profile_tool],  # list of tools for the agent
            name="get_flavor_profile_via_a2a",
        )

        graph = create_supervisor(
            model=model,
            agents=[get_flavor_profile_a2a_agent],  # worker agents list
            prompt=(
            "You are a routing-only supervisor agent. You are never allowed to answer user questions yourself.\n"
            "Your behavior is strictly rule-based and must follow this logic:\n"
            "1. If the user prompt includes anything about coffee flavor, taste, or sensory profile:\n"
            "    - Route the task to worker agent 'get_flavor_profile_via_a2a'\n"
            "    - Use the associated tool `flavor_profile_tool`\n"
            "    - Do not answer or describe anything about coffee flavor\n"
            "2. If the user prompt is not about flavor, taste, or sensory profile:\n"
            "    - Respond with this exact message:\n"
            "      \"I'm sorry, I cannot assist with that request. Please ask about coffee flavor or taste.\"\n"
            "3. If the worker agent returns control and the result is successful with no errors:\n"
            "    - Return an empty response and end the conversation\n"
            "4. If the worker agent returns an error:\n"
            "    - Return the same error message verbatim\n"
            "\n"
            "You must never generate any original content, answers, or descriptions.\n"
            "If you fail to match the user's input to rule 1, default to rule 2.\n"
            ),
            add_handoff_back_messages=False,
            output_mode="last_message",
        ).compile()
        logger.debug("LangGraph supervisor created and compiled successfully.")
        return graph

    async def serve(self, prompt: str):
        """
        Processes the input prompt and returns a response from the graph.
        Args:
            prompt (str): The input prompt to be processed by the graph.
        Returns:
            str: The response generated by the graph based on the input prompt.
        """
        try:
            # build graph if not already built
            if not hasattr(self, 'graph'):
                self.graph = self.build_graph()
            logger.debug(f"Received prompt: {prompt}")
            if not isinstance(prompt, str) or not prompt.strip():
                raise ValueError("Prompt must be a non-empty string.")
            # session_start()
            result = await self.graph.ainvoke({
                "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
                ],
            }, {"configurable": {"thread_id": uuid.uuid4()}})

            messages = result.get("messages", [])
            if not messages:
                raise RuntimeError("No messages found in the graph response.")

            # Find the last AIMessage with non-empty content
            for message in reversed(messages):
                if isinstance(message, AIMessage) and message.content.strip():
                    logger.debug(f"Valid AIMessage found: {message.content.strip()}")
                    return message.content.strip()

            raise RuntimeError("No valid AIMessage found in the graph response.")
        except ValueError as ve:
            logger.error(f"ValueError in serve method: {ve}")
            raise ValueError(str(ve))
        except Exception as e:
            logger.error(f"Error in serve method: {e}")
            raise Exception(str(e))
